{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":9168081,"sourceType":"datasetVersion","datasetId":5539900},{"sourceId":9174466,"sourceType":"datasetVersion","datasetId":5544559}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"\n!pip install -q accelerate peft bitsandbytes transformers trl nlpaug","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-17T10:13:05.822013Z","iopub.execute_input":"2024-08-17T10:13:05.822732Z","iopub.status.idle":"2024-08-17T10:13:19.424645Z","shell.execute_reply.started":"2024-08-17T10:13:05.822697Z","shell.execute_reply":"2024-08-17T10:13:19.423334Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\nimport os\nimport torch\nfrom datasets import Dataset, DatasetDict, concatenate_datasets\nfrom transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    TrainingArguments,\n    pipeline,\n    logging,\n    TrainerCallback\n)\nfrom peft import LoraConfig\nfrom trl import SFTTrainer\nimport pandas as pd\nimport nlpaug.augmenter.word as naw\nimport gc\nfrom torch.nn import DataParallel","metadata":{"execution":{"iopub.status.busy":"2024-08-17T10:13:19.426810Z","iopub.execute_input":"2024-08-17T10:13:19.427178Z","iopub.status.idle":"2024-08-17T10:14:01.705795Z","shell.execute_reply.started":"2024-08-17T10:13:19.427148Z","shell.execute_reply":"2024-08-17T10:14:01.705003Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"2024-08-17 10:13:26.017351: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-17 10:13:26.017411: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-17 10:13:26.020484: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"from huggingface_hub import notebook_login\nnotebook_login()","metadata":{"execution":{"iopub.status.busy":"2024-08-17T10:14:01.707779Z","iopub.execute_input":"2024-08-17T10:14:01.709014Z","iopub.status.idle":"2024-08-17T10:14:01.733971Z","shell.execute_reply.started":"2024-08-17T10:14:01.708976Z","shell.execute_reply":"2024-08-17T10:14:01.732925Z"},"trusted":true},"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/plain":"VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a6e2bae9408a48ee875c8a7a2bd670be"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Define the LogMetricsCallback class\nclass LogMetricsCallback(TrainerCallback):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n        # Write the header to the file\n        with open(self.output_file, \"w\") as f:\n            f.write(\"step,epoch,train_loss,eval_loss,learning_rate\\n\")\n\n    def on_log(self, args, state, control, **kwargs):\n        # Get the current metrics\n        step = state.global_step\n        epoch = state.epoch\n        train_loss = state.log_history[-1].get(\"loss\", \"N/A\")\n        eval_loss = state.log_history[-1].get(\"eval_loss\", \"N/A\")\n        learning_rate = state.log_history[-1].get(\"learning_rate\", \"N/A\")\n\n        # Append metrics to the file\n        with open(self.output_file, \"a\") as f:\n            f.write(f\"{step},{epoch},{train_loss},{eval_loss},{learning_rate}\\n\")\n\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-17T10:14:45.826906Z","iopub.execute_input":"2024-08-17T10:14:45.827638Z","iopub.status.idle":"2024-08-17T10:14:45.835778Z","shell.execute_reply.started":"2024-08-17T10:14:45.827601Z","shell.execute_reply":"2024-08-17T10:14:45.834759Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Constants and configurations\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nNEW_MODEL = \"llama-2-7b-user-manuals-new\"\nMAX_SEQ_LENGTH = 128\nOUTPUT_DIR = \"./results\"\n\n\n# LoRA parameters\nLORA_R = 16\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.3\n\n# Training arguments\nTRAIN_ARGS = {\n    \"output_dir\": OUTPUT_DIR,\n    \"num_train_epochs\": 3,\n    \"per_device_train_batch_size\": 4,\n    \"per_device_eval_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"max_grad_norm\": 0.3,\n    \"learning_rate\": 2e-4,\n    \"weight_decay\": 0.1,\n    \"optim\": \"paged_adamw_32bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"group_by_length\": True,\n    \"logging_steps\": 16,\n    \"save_steps\" : 32,\n    \"eval_steps\" : 32,\n    \"evaluation_strategy\": \"steps\",\n    \"save_strategy\": \"steps\",\n    \"load_best_model_at_end\": True,\n    \"report_to\": \"tensorboard\",\n}\n\n# Bits and Bytes configuration\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Setup model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=BNB_CONFIG,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Setup LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    r=LORA_R,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Prepare dataset\ndf = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\ndf[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\ndf = df.drop(columns=['Instructions', 'Responses'])\n# Shuffle the dataframe\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\n# Split the data into train and validation\ntrain_df = df.sample(frac=0.8, random_state=42,)\nval_df = df.drop(train_df.index)\n\n# Convert the training data into a Dataset\ntrain_dataset = Dataset.from_pandas(train_df,)\n\n# Define your augmenters\ncontextual_augmenter = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"substitute\")\nkeyboard_error_augmenter = naw.SpellingAug()\n\ndef contextual_augmentation(examples):\n    augmented_texts = []\n    for text in examples['formatted_instruction']:\n        inst_start = text.find(\"[INST]\") + 6\n        inst_end = text.find(\"[/INST]\")\n        if inst_start != -1 and inst_end != -1:\n            instruction = text[inst_start:inst_end].strip()\n            augmented_instruction = contextual_augmenter.augment(instruction)[0]\n            augmented_text = text[:inst_start] + augmented_instruction + text[inst_end:]\n            augmented_texts.append(augmented_text)\n        else:\n            augmented_texts.append(text)\n    return {'formatted_instruction': augmented_texts}\n\ndef keyboard_error_augmentation(examples):\n    augmented_texts = []\n    for text in examples['formatted_instruction']:\n        inst_start = text.find(\"[INST]\") + 6\n        inst_end = text.find(\"[/INST]\")\n        if inst_start != -1 and inst_end != -1:\n            instruction = text[inst_start:inst_end].strip()\n            augmented_instructions = keyboard_error_augmenter.augment(instruction)\n            # If the augmentation returns a list, take the first item\n            augmented_instruction = augmented_instructions[0] if augmented_instructions else instruction\n            augmented_text = text[:inst_start] + augmented_instruction + text[inst_end:]\n            augmented_texts.append(augmented_text)\n        else:\n            augmented_texts.append(text)\n    return {'formatted_instruction': augmented_texts}\n\n# Perform contextual word embedding augmentation\ncontextual_augmented_dataset = train_dataset.map(\n    contextual_augmentation,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)\n\n# Perform keyboard error augmentation\nkeyboard_error_augmented_dataset = train_dataset.map(\n    keyboard_error_augmentation,\n    batched=True,\n    remove_columns=train_dataset.column_names\n)\n\n# Concatenate the original, contextual augmented, and keyboard error augmented datasets\ncombined_train_dataset = concatenate_datasets([\n    train_dataset,\n    contextual_augmented_dataset,\n    keyboard_error_augmented_dataset\n])\n\n# Shuffle the final combined dataset\ncombined_train_dataset = combined_train_dataset.shuffle(seed=42)\n\n# Convert the validation data into a Dataset (no augmentation)\nval_dataset = Dataset.from_pandas(val_df)\n\n# Setup training arguments\ntraining_args = TrainingArguments(**TRAIN_ARGS)\n\n# Initialize the custom callback\nlog_callback = LogMetricsCallback(output_file=\"./logs/training_metrics.csv\")\n\n# Initialize trainer with the custom callback\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=combined_train_dataset,\n    eval_dataset=val_dataset,\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_instruction\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n    callbacks=[log_callback],\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.model.save_pretrained(NEW_MODEL)\n\nprint(\"Training completed.\")","metadata":{"execution":{"iopub.status.busy":"2024-08-17T10:14:52.184494Z","iopub.execute_input":"2024-08-17T10:14:52.185298Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e93b0dbf591434b8b52a22825b4f16d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec24c0c627d14bbda12cfb63cee1910c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/110 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7455f0fea2744cb6aacfb465d997fe77"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': dataset_text_field, max_seq_length. Will not be supported from version '1.0.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:280: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:318: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/330 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8983b8aad5644369a51e509c602d5cb7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/28 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c8d399074b2c4eb3aada7ffcd4be78e0"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='110' max='249' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [110/249 10:24 < 13:24, 0.17 it/s, Epoch 1.31/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>32</td>\n      <td>2.692200</td>\n      <td>2.315817</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>2.369900</td>\n      <td>2.175300</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>1.789800</td>\n      <td>2.143715</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66c07917-4f0d312c703ec85c733358db;807bf5b7-63d8-4574-bd4f-88801ac98709)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66c079c5-36284aa3134987273809cbd8;c10830b4-7caf-4f5e-b9c3-7faed6e9884c)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/other.py:619: UserWarning: Unable to fetch remote file due to the following error 401 Client Error. (Request ID: Root=1-66c07a85-4ebf10b324c150775b5d4b21;61b3353a-492a-4702-ac89-392d2236c4a8)\n\nCannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\nAccess to model meta-llama/Llama-2-7b-hf is restricted. You must be authenticated to access it. - silently ignoring the lookup for the file config.json in meta-llama/Llama-2-7b-hf.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/peft/utils/save_and_load.py:218: UserWarning: Could not find a config file in meta-llama/Llama-2-7b-hf - will assume that the vocabulary was not modified.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/torch/utils/checkpoint.py:429: UserWarning: torch.utils.checkpoint: please pass in use_reentrant=True or use_reentrant=False explicitly. The default value of use_reentrant will be updated to be False in the future. To maintain current behavior, pass use_reentrant=True. It is recommended that you use use_reentrant=False. Refer to docs for more details on the differences between the two variants.\n  warnings.warn(\n","output_type":"stream"}]},{"cell_type":"code","source":"import gc\ngc.collect()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Run text generation pipeline with our next model\nprompt = \"What is the info about Gateway in IP configuration with respect to laser PLC access?\"\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import shutil\n\nsrc_path = r\"/kaggle/input/llama2\"\ndst_path = r\"/kaggle/working/llama2\"\n\nshutil.copytree(src_path, dst_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import logging\nfrom transformers import pipeline, AutoModelForCausalLM, AutoTokenizer\nfrom peft import PeftModel\n\n# Load the base model\nbase_model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Load the tokenizer\ntokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\")\n\n# Load the adapter model\nadapter_model_id = \"/kaggle/working/llama2\"\nmodel = PeftModel.from_pretrained(base_model, adapter_model_id, is_trainable=False)\n\n# Set logging level to ignore warnings\nlogging.getLogger().setLevel(logging.CRITICAL)\n\n# Define the prompt\nprompt = \"What is the info about Gateway in IP configuration?\"\n\n# Create the text generation pipeline\npipe = pipeline(task=\"text-generation\", model=model, tokenizer=tokenizer, max_length=200)\n\n# Generate text\nresult = pipe(f\"<s>[INST] {prompt} [/INST]\")\n\n# Print the generated text\nprint(result[0]['generated_text'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# '\n# df = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\n# df[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\n# #df = df.drop(columns=df.columns[:2])\n# df = df.drop(columns=['Instructions', 'Responses'])\n# df.to_csv(\"conversational_genAI.csv\", index=False)\n# #created_dataset = Dataset.from_pandas(df)\n# #print(created_dataset)\n\n\n# df_read = pd.read_csv(\"/kaggle/working/conversational_genAI.csv\")\n\n# # Ensure the key 'formatted_instruction' exists in the DataFrame\n# if \"formatted_instruction\" not in df_read.columns:\n#     raise ValueError(\"The key 'formatted_instruction' does not exist in the DataFrame\")\n\n# # Convert DataFrame to Hugging Face Dataset\n# dataset = Dataset.from_pandas(df_read)\n\n# train_size = 0.8\n# train_dataset = df_read.sample(frac=train_size, random_state=42)\n# val_dataset = df_read.drop(train_dataset.index)\n \n# train_dataset = Dataset.from_pandas(train_dataset)\n# val_dataset = Dataset.from_pandas(val_dataset)\n\n# # Create a DatasetDict with the train split\n# dataset_dict = DatasetDict({\n#     \"train\": train_dataset,\n#     \"validation\" : val_dataset\n# })\n# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# # model name\n# model_name = \"meta-llama/Llama-2-7b-hf\"\n# new_model = \"llama-2-7b-user-manuals-new\"\n\n# max_seq_length = 128                    # Maximum sequence length to use\n# packing = False                        # Pack multiple short examples in the same input sequence to increase efficiency\n# device_map = {\"\": 0}                   # Load the entire model on the GPU 0\n\n\n# # Tokenize the dataset (example tokenization step)\n# def tokenize_function(examples):\n#     return tokenizer(examples[\"formatted_instruction\"], padding=\"max_length\", truncation=True, max_length=max_seq_length)\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True,device_map = 'auto')\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n# tokenized_dataset = dataset_dict.map(tokenize_function, batched=True)\n\n# #dataset_name = \"/content/conversational_genAI.csv\"\n# #def tokenize(example):\n# #    return tokenizer(example['formatted_instruction'], truncation=True, padding='max_length',max_length=max_seq_length)\n\n\n# #tokenized_dataset = created_dataset.map(tokenize, batched=True, remove_columns=created_dataset.column_names)\n# #print(tokenized_dataset[0])'","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# import pandas as pd\n# from datasets import Dataset, DatasetDict\n# from transformers import AutoTokenizer, AutoModelForCausalLM\n# import random\n# import torch\n\n# # Load the model and tokenizer\n# model_name = \"meta-llama/Llama-2-7b-hf\"\n# new_model = \"llama-2-7b-user-manuals-new\"\n# max_seq_length = 128\n# packing = False\n# device_map = {\"\": 0}\n\n# tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True, device_map='auto')\n# tokenizer.pad_token = tokenizer.eos_token\n# tokenizer.padding_side = \"right\"\n\n# model = AutoModelForCausalLM.from_pretrained(model_name, device_map=device_map)\n\n# def paraphrase_word(sentence, word_to_paraphrase, model, tokenizer):\n#     masked_sentence = sentence.replace(word_to_paraphrase, \"[MASK]\")\n#     inputs = tokenizer(masked_sentence, return_tensors=\"pt\").to(model.device)\n    \n#     with torch.no_grad():\n#         outputs = model(**inputs)\n    \n#     mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n#     predicted_token_id = outputs.logits[0, mask_token_index].argmax(axis=-1)\n#     predicted_word = tokenizer.decode(predicted_token_id)\n    \n#     return predicted_word.strip()\n\n# def augment_question(question):\n#     # Extract the instruction part\n#     inst_start = question.find(\"[INST]\") + 6\n#     inst_end = question.find(\"[/INST]\")\n#     instruction = question[inst_start:inst_end].strip()\n    \n#     words = instruction.split()\n#     if len(words) < 3:  # Skip very short instructions\n#         return question\n    \n#     word_to_paraphrase = random.choice(words)\n#     paraphrased_word = paraphrase_word(instruction, word_to_paraphrase, model, tokenizer)\n    \n#     augmented_instruction = instruction.replace(word_to_paraphrase, paraphrased_word, 1)\n    \n#     # Reconstruct the full question with augmented instruction\n#     return question[:inst_start] + augmented_instruction + question[inst_end:]\n\n# def tokenize_and_augment(examples):\n#     augmented_instructions = [augment_question(instruction) for instruction in examples[\"formatted_instruction\"]]\n#     return tokenizer(augmented_instructions, padding=\"max_length\", truncation=True, max_length=max_seq_length)\n\n# # Load and preprocess the data\n# df = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\n# df[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\n# df = df.drop(columns=['Instructions', 'Responses'])\n# df.to_csv(\"conversational_genAI.csv\", index=False)\n\n# df_read = pd.read_csv(\"/kaggle/working/conversational_genAI.csv\")\n\n# # Split the data\n# train_size = 0.8\n# train_df = df_read.sample(frac=train_size, random_state=42)\n# val_df = df_read.drop(train_df.index)\n\n# # Convert to Hugging Face Datasets\n# train_dataset = Dataset.from_pandas(train_df)\n# val_dataset = Dataset.from_pandas(val_df)\n\n# # Create a DatasetDict\n# dataset_dict = DatasetDict({\n#     \"train\": train_dataset,\n#     \"validation\": val_dataset\n# })\n\n# # Apply tokenization and augmentation\n# tokenized_dataset = dataset_dict.map(tokenize_and_augment, batched=True, remove_columns=dataset_dict[\"train\"].column_names)\n\n# # The tokenized_dataset is now ready for use in your training pipeline","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q nlpaug","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Below code is working for data augmentatio using nlpaug library**\n****","metadata":{}},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch\nimport gc\nimport pandas as pd\nfrom sklearn.model_selection import KFold\n\n# Constants and configurations\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nNEW_MODEL = \"llama-2-7b-user-manuals-new\"\nMAX_SEQ_LENGTH = 128\nOUTPUT_DIR = \"./results\"\nN_SPLITS = 5  # Number of folds for K-Fold Cross-Validation\n\n# LoRA parameters\nLORA_R = 16\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.1 #0.5\n\n# Training arguments\nTRAIN_ARGS = {\n    \"output_dir\": OUTPUT_DIR,\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 4,\n    \"per_device_eval_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"max_grad_norm\": 0.3,\n    \"learning_rate\": 2e-4,\n    \"weight_decay\": 0.01,\n    \"optim\": \"paged_adamw_32bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"group_by_length\": True,\n    \"logging_steps\": 5,\n    \"evaluation_strategy\": \"epoch\",\n    \"save_strategy\": \"epoch\",\n    \"load_best_model_at_end\": True,\n    \"report_to\": \"tensorboard\",\n}\n\n# Bits and Bytes configuration\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Setup model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=BNB_CONFIG,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load dataset\ndf = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\ndf[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\ndf = df.drop(columns=['Instructions', 'Responses'])\n\n# Initialize NLPAug augmenter\naugmenter = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"substitute\")\n\ndef augment_instruction(instruction):\n    augmented = augmenter.augment(instruction)\n    if isinstance(augmented, list):\n        augmented = \" \".join(augmented)\n    return augmented\n\ndef augment_question(question):\n    inst_start = question.find(\"[INST]\") + 6\n    inst_end = question.find(\"[/INST]\")\n    instruction = question[inst_start:inst_end].strip()\n    \n    augmented_instruction = augment_instruction(instruction)\n    \n    return question[:inst_start] + augmented_instruction + question[inst_end:]\n\ndef tokenize_and_augment(examples):\n    augmented_instructions = [augment_question(instruction) for instruction in examples[\"formatted_instruction\"]]\n    return tokenizer(augmented_instructions, padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH)\n\n# Setup K-Fold Cross-Validation\nkf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=42)\n\nfor fold, (train_index, val_index) in enumerate(kf.split(df)):\n    print(f\"Starting fold {fold + 1}/{N_SPLITS}\")\n\n    # Split the data into train and validation\n    train_df = df.iloc[train_index]\n    val_df = df.iloc[val_index]\n\n    # Augment training data\n    train_dataset = Dataset.from_pandas(train_df)\n    tokenized_train_dataset = train_dataset.map(\n        tokenize_and_augment,\n        batched=True,\n        remove_columns=train_dataset.column_names\n    )\n    \n    # No augmentation for validation data\n    val_dataset = Dataset.from_pandas(val_df)\n    tokenized_val_dataset = val_dataset.map(\n        lambda examples: tokenizer(examples[\"formatted_instruction\"], padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH),\n        batched=True,\n        remove_columns=val_dataset.column_names\n    )\n\n    # Create DatasetDict for this fold\n    dataset_dict = DatasetDict({\n        \"train\": tokenized_train_dataset,\n        \"validation\": tokenized_val_dataset\n    })\n\n    # Setup LoRA configuration\n    peft_config = LoraConfig(\n        lora_alpha=LORA_ALPHA,\n        lora_dropout=LORA_DROPOUT,\n        r=LORA_R,\n        bias=\"none\",\n        task_type=\"CAUSAL_LM\",\n    )\n\n    # Setup training arguments\n    training_args = TrainingArguments(**TRAIN_ARGS)\n\n    # Initialize trainer\n    trainer = SFTTrainer(\n        model=model,\n        train_dataset=dataset_dict[\"train\"],\n        eval_dataset=dataset_dict[\"validation\"],\n        peft_config=peft_config,\n        dataset_text_field=\"formatted_instruction\",\n        max_seq_length=MAX_SEQ_LENGTH,\n        tokenizer=tokenizer,\n        args=training_args,\n    )\n\n    # Train the model\n    trainer.train()\n\n    # Optionally, save model after each fold\n    fold_model_dir = f\"{NEW_MODEL}_fold_{fold + 1}\"\n    trainer.model.save_pretrained(fold_model_dir)\n\n    print(f\"Finished fold {fold + 1}/{N_SPLITS}\")\n\n# Optionally, aggregate results across folds\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":" **below code is the same script as above in addition to printing the first 5 examples of before and after augmentation******","metadata":{}},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch\nimport gc\nimport pandas as pd\n\n# Constants and configurations\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nNEW_MODEL = \"llama-2-7b-user-manuals-new\"\nMAX_SEQ_LENGTH = 128\nOUTPUT_DIR = \"./results\"\n\n# LoRA parameters\nLORA_R = 16\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.1\n\n# Training arguments\nTRAIN_ARGS = {\n    \"output_dir\": OUTPUT_DIR,\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 4,\n    \"per_device_eval_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"max_grad_norm\": 0.3,\n    \"learning_rate\": 2e-4,\n    \"weight_decay\": 0.01,\n    \"optim\": \"paged_adamw_32bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"group_by_length\": True,\n    \"logging_steps\": 5,\n    \"evaluation_strategy\": \"epoch\",\n    \"save_strategy\": \"epoch\",\n    \"load_best_model_at_end\": True,\n    \"report_to\": \"tensorboard\",\n}\n\n# Bits and Bytes configuration\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Setup model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=BNB_CONFIG,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Prepare dataset\ndf = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\ndf[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\ndf = df.drop(columns=['Instructions', 'Responses'])\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\ndataset_dict = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df),\n    \"validation\": Dataset.from_pandas(val_df)\n})\n\n# Initialize NLPAug augmenter\naugmenter = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"substitute\")\n\ndef augment_instruction(instruction):\n    # NLPAug returns a list, so join it into a single string\n    augmented = augmenter.augment(instruction)\n    if isinstance(augmented, list):\n        augmented = \" \".join(augmented)  # Join the list into a single string\n    return augmented\n\ndef augment_question(question):\n    # Extract the instruction part\n    inst_start = question.find(\"[INST]\") + 6\n    inst_end = question.find(\"[/INST]\")\n    instruction = question[inst_start:inst_end].strip()\n    \n    augmented_instruction = augment_instruction(instruction)\n    \n    # Reconstruct the full question with augmented instruction\n    return question[:inst_start] + augmented_instruction + question[inst_end:]\n\ndef tokenize_and_augment(examples):\n    # Print original instructions\n    print(\"Original instructions:\")\n    print(examples[\"formatted_instruction\"][:5])  # Print first 5 examples for brevity\n    \n    # Augment instructions\n    augmented_instructions = [augment_question(instruction) for instruction in examples[\"formatted_instruction\"]]\n    \n    # Print augmented instructions\n    print(\"Augmented instructions:\")\n    print(augmented_instructions[:5])  # Print first 5 examples for brevity\n    \n    # Tokenize augmented instructions\n    return tokenizer(augmented_instructions, padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH)\n\n# Tokenize and augment dataset\ntokenized_dataset = dataset_dict.map(\n    tokenize_and_augment,\n    batched=True,\n    remove_columns=dataset_dict[\"train\"].column_names\n)\n\n# Setup LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    r=LORA_R,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Setup training arguments\ntraining_args = TrainingArguments(**TRAIN_ARGS)\n\n# Initialize trainer\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_instruction\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.model.save_pretrained(NEW_MODEL)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Below code is with loss function and their .logfiles","metadata":{}},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, TrainerCallback\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch\nimport gc\nimport pandas as pd\nimport os\n\n# Define the LogMetricsCallback class\nclass LogMetricsCallback(TrainerCallback):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n        # Write the header to the file\n        with open(self.output_file, \"w\") as f:\n            f.write(\"step,epoch,train_loss,eval_loss,learning_rate\\n\")\n\n    def on_log(self, args, state, control, **kwargs):\n        # Get the current metrics\n        step = state.global_step\n        epoch = state.epoch\n        train_loss = state.log_history[-1].get(\"loss\", \"N/A\")\n        eval_loss = state.log_history[-1].get(\"eval_loss\", \"N/A\")\n        learning_rate = state.log_history[-1].get(\"learning_rate\", \"N/A\")\n\n        # Append metrics to the file\n        with open(self.output_file, \"a\") as f:\n            f.write(f\"{step},{epoch},{train_loss},{eval_loss},{learning_rate}\\n\")\n\n\n# Constants and configurations\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nNEW_MODEL = \"llama-2-7b-user-manuals-new\"\nMAX_SEQ_LENGTH = 128\nOUTPUT_DIR = \"./results\"\n\n# LoRA parameters\nLORA_R = 16\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.1\n\n# Training arguments\nTRAIN_ARGS = {\n    \"output_dir\": OUTPUT_DIR,\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 4,\n    \"per_device_eval_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"max_grad_norm\": 0.3,\n    \"learning_rate\": 2e-4,\n    \"weight_decay\": 0.01,\n    \"optim\": \"paged_adamw_32bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"group_by_length\": True,\n    \"logging_steps\": 5,\n    \"evaluation_strategy\": \"steps\",\n    \"save_strategy\": \"steps\",\n    \"load_best_model_at_end\": True,\n    \"report_to\": \"tensorboard\",\n}\n\n# Bits and Bytes configuration\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Setup model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=BNB_CONFIG,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Prepare dataset\ndf = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\ndf[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\ndf = df.drop(columns=['Instructions', 'Responses'])\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\ndataset_dict = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df),\n    \"validation\": Dataset.from_pandas(val_df)\n})\n\n# Initialize NLPAug augmenter\naugmenter = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"substitute\")\n\ndef augment_instruction(instruction):\n    # NLPAug returns a list, so join it into a single string\n    augmented = augmenter.augment(instruction)\n    if isinstance(augmented, list):\n        augmented = \" \".join(augmented)  # Join the list into a single string\n    return augmented\n\ndef augment_question(question):\n    # Extract the instruction part\n    inst_start = question.find(\"[INST]\") + 6\n    inst_end = question.find(\"[/INST]\")\n    instruction = question[inst_start:inst_end].strip()\n    \n    augmented_instruction = augment_instruction(instruction)\n    \n    # Reconstruct the full question with augmented instruction\n    return question[:inst_start] + augmented_instruction + question[inst_end:]\n\ndef tokenize_and_augment(examples):\n    # Print original instructions\n    print(\"Original instructions:\")\n    print(examples[\"formatted_instruction\"][:5])  # Print first 5 examples for brevity\n    \n    # Augment instructions\n    augmented_instructions = [augment_question(instruction) for instruction in examples[\"formatted_instruction\"]]\n    \n    # Print augmented instructions\n    print(\"Augmented instructions:\")\n    print(augmented_instructions[:5])  # Print first 5 examples for brevity\n    \n    # Tokenize augmented instructions\n    return tokenizer(augmented_instructions, padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH)\n\n# Tokenize and augment dataset\ntokenized_dataset = dataset_dict.map(\n    tokenize_and_augment,\n    batched=True,\n    remove_columns=dataset_dict[\"train\"].column_names\n)\n\n# Setup LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    r=LORA_R,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Setup training arguments\ntraining_args = TrainingArguments(**TRAIN_ARGS)\n\n# Initialize the custom callback\nlog_callback = LogMetricsCallback(output_file=\"./logs/training_metrics.csv\")\n\n# Initialize trainer with the custom callback\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_instruction\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n    callbacks=[log_callback],  # Add the custom callback here\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.model.save_pretrained(NEW_MODEL)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"pip install -q nlpaug","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nlpaug.augmenter.word as naw\nfrom datasets import Dataset, DatasetDict\nfrom transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, BitsAndBytesConfig, TrainerCallback\nfrom peft import LoraConfig, get_peft_model\nfrom trl import SFTTrainer\nimport torch\nimport gc\nimport pandas as pd\nimport os\n\n# Define the LogMetricsCallback class\nclass LogMetricsCallback(TrainerCallback):\n    def __init__(self, output_file):\n        self.output_file = output_file\n\n        # Ensure the directory exists\n        os.makedirs(os.path.dirname(output_file), exist_ok=True)\n\n        # Write the header to the file\n        with open(self.output_file, \"w\") as f:\n            f.write(\"step,epoch,train_loss,eval_loss,learning_rate\\n\")\n\n    def on_log(self, args, state, control, **kwargs):\n        # Get the current metrics\n        step = state.global_step\n        epoch = state.epoch\n        train_loss = state.log_history[-1].get(\"loss\", \"N/A\")\n        eval_loss = state.log_history[-1].get(\"eval_loss\", \"N/A\")\n        learning_rate = state.log_history[-1].get(\"learning_rate\", \"N/A\")\n\n        # Append metrics to the file\n        with open(self.output_file, \"a\") as f:\n            f.write(f\"{step},{epoch},{train_loss},{eval_loss},{learning_rate}\\n\")\n\n\n# Constants and configurations\nMODEL_NAME = \"meta-llama/Llama-2-7b-hf\"\nNEW_MODEL = \"llama-2-7b-user-manuals-new\"\nMAX_SEQ_LENGTH = 128\nOUTPUT_DIR = \"./results\"\n\n# LoRA parameters\nLORA_R = 16\nLORA_ALPHA = 64\nLORA_DROPOUT = 0.1\n\n# Training arguments\nTRAIN_ARGS = {\n    \"output_dir\": OUTPUT_DIR,\n    \"num_train_epochs\": 1,\n    \"per_device_train_batch_size\": 4,\n    \"per_device_eval_batch_size\": 4,\n    \"gradient_accumulation_steps\": 1,\n    \"gradient_checkpointing\": True,\n    \"max_grad_norm\": 0.3,\n    \"learning_rate\": 2e-4,\n    \"weight_decay\": 0.01,\n    \"optim\": \"paged_adamw_32bit\",\n    \"lr_scheduler_type\": \"cosine\",\n    \"warmup_ratio\": 0.03,\n    \"group_by_length\": True,\n    \"logging_steps\": 5,\n    \"evaluation_strategy\": \"steps\",\n    \"save_strategy\": \"steps\",\n    \"load_best_model_at_end\": True,\n    \"report_to\": \"tensorboard\",\n}\n\n# Bits and Bytes configuration\nBNB_CONFIG = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch.float16,\n    bnb_4bit_use_double_quant=False,\n)\n\n# Clear CUDA cache\ngc.collect()\ntorch.cuda.empty_cache()\n\n# Setup model and tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, trust_remote_code=True)\ntokenizer.pad_token = tokenizer.eos_token\ntokenizer.padding_side = \"right\"\n\nmodel = AutoModelForCausalLM.from_pretrained(\n    MODEL_NAME,\n    quantization_config=BNB_CONFIG,\n    device_map=\"auto\",\n    trust_remote_code=True,\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Prepare dataset\ndf = pd.read_excel('/kaggle/input/dataset/Dataset_creation.xlsx')\ndf[\"formatted_instruction\"] = df.apply(lambda x: f\"<s>[INST] {x['Instructions']} [/INST] {x['Responses']} </s>\", axis=1)\ndf = df.drop(columns=['Instructions', 'Responses'])\n\ntrain_df = df.sample(frac=0.8, random_state=42)\nval_df = df.drop(train_df.index)\n\ndataset_dict = DatasetDict({\n    \"train\": Dataset.from_pandas(train_df),\n    \"validation\": Dataset.from_pandas(val_df)\n})\n\n# Initialize NLPAug augmenter\naugmenter = naw.ContextualWordEmbsAug(model_path='distilbert-base-uncased', action=\"substitute\")\nkeyboard_error_augmenter = naw.SpellingAug()  # Assuming this introduces keyboard errors\n\ndef augment_instruction(instruction):\n    # Apply character-based augmentation for keyboard errors\n    augmented = keyboard_error_augmenter.augment(instruction, n=3)  # Generate multiple augmented versions\n    # Join all augmented texts into a single string, or select one\n    if isinstance(augmented, list):\n        return \" \".join(augmented)  # Join the list into a single string\n    return augmented\n\ndef augment_question(question):\n    # Extract the instruction part\n    inst_start = question.find(\"[INST]\") + 6\n    inst_end = question.find(\"[/INST]\")\n    instruction = question[inst_start:inst_end].strip()\n    \n    # Augment the instruction\n    augmented_instruction = augment_instruction(instruction)\n    \n    # Reconstruct the full question with augmented instruction\n    return question[:inst_start] + augmented_instruction + question[inst_end:]\n\ndef tokenize_and_augment(examples):\n    # Print original instructions\n    print(\"Original instructions:\")\n    print(examples[\"formatted_instruction\"][:5])  # Print first 5 examples for brevity\n    \n    # Augment instructions\n    augmented_instructions = [augment_question(instruction) for instruction in examples[\"formatted_instruction\"]]\n    \n    # Print augmented instructions\n    print(\"Augmented instructions:\")\n    print(augmented_instructions[:5])  # Print first 5 examples for brevity\n    \n    # Tokenize augmented instructions\n    return tokenizer(augmented_instructions, padding=\"max_length\", truncation=True, max_length=MAX_SEQ_LENGTH)\n\n# Tokenize and augment dataset\ntokenized_dataset = dataset_dict.map(\n    tokenize_and_augment,\n    batched=True,\n    remove_columns=dataset_dict[\"train\"].column_names\n)\n# Setup LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=LORA_ALPHA,\n    lora_dropout=LORA_DROPOUT,\n    r=LORA_R,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n\n# Setup training arguments\ntraining_args = TrainingArguments(**TRAIN_ARGS)\n\n# Initialize the custom callback\nlog_callback = LogMetricsCallback(output_file=\"./logs/training_metrics.csv\")\n\n# Initialize trainer with the custom callback\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_instruction\",\n    max_seq_length=MAX_SEQ_LENGTH,\n    tokenizer=tokenizer,\n    args=training_args,\n    callbacks=[log_callback],  # Add the custom callback here\n)\n\n# Train the model\ntrainer.train()\n\n# Save the model\ntrainer.model.save_pretrained(NEW_MODEL)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pandas as pd\n\n# Read the CSV file\ndf = pd.read_csv('/kaggle/input/log-file/training_metrics.csv')\n\n# Convert columns to individual arrays\nsteps = df['step'].tolist()\nepochs = df['epoch'].tolist()\ntrain_loss = df['train_loss'].tolist()\neval_loss = df['eval_loss'].tolist()\nlearning_rate = df['learning_rate'].tolist()\n\n# Print original arrays\nprint(\"Original arrays:\")\nprint(\"Steps:\", steps)\nprint(\"Epochs:\", epochs)\nprint(\"Train Loss:\", train_loss)\nprint(\"Eval Loss:\", eval_loss)\nprint(\"Learning Rate:\", learning_rate)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_loss_filtered = [t for t in train_loss if pd.notna(t)]\n\neval_loss_filtered = [eval_loss[i] for i, e in enumerate(eval_loss) if pd.notna(e)]\nlearning_rate_filtered = [learning_rate[i] for i, l in enumerate(learning_rate) if pd.notna(l)]\nunique_steps = list(set(steps))\nunique_steps.sort()  # Optional: sort the list if needed\nunique_epochs = list(set(epochs))\nunique_epochs.sort()  # Optional: sort the list if needed\n\n# Remove the last value from each list\nif unique_steps:  # Check if the list is not empty\n    unique_steps.pop()  # Remove the last value\n\nif unique_epochs:  # Check if the list is not empty\n    unique_epochs.pop()  # Remove the last value\n\n# Print the updated lists\nprint(\"Updated Unique Steps:\", unique_steps)\nprint(\"Updated Unique Epochs:\", unique_epochs)\n\nprint(\"Train Loss:\", train_loss_filtered)\nprint(\"Eval Loss:\", eval_loss_filtered)\nprint(\"Learning Rate:\", learning_rate_filtered)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\n\n# Plot Train Loss vs Steps\nplt.figure(figsize=(12, 6))\n\n# First plot: Train Loss vs Steps\nplt.subplot(1, 2, 1)\nplt.plot(train_loss_filtered, unique_steps, marker='o', label='Train Loss')\nplt.plot(eval_loss_filtered, unique_steps, marker='x', label='Eval Loss')\nplt.xlabel('Loss')\nplt.ylabel('global Steps')\nplt.title('Train Loss and Eval Loss vs Steps')\nplt.legend()\nplt.grid(True)\n\n# Second plot: Learning Rate vs Train Loss\nplt.subplot(1, 2, 2)\nplt.plot(train_loss_filtered, learning_rate_filtered, marker='o', color='r', label='Train Loss vs Learning Rate')\nplt.xlabel('Train Loss')\nplt.ylabel('Learning Rate')\nplt.title('Learning Rate vs Train Loss')\nplt.legend()\nplt.grid(True)\n\nplt.tight_layout()\nplt.show()\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#-----QLora parameters------#\nlora_r = 16            # attention mechanism\nlora_alpha = 64        # scaling purpose\nlora_dropout = 0.1     # dropout probaility layer\n\n#----Bits and bytes parameters-------#\nload_4bit=True                      # Activate 4-bit precision base model loading\nbnb_4bit_quant_type=\"nf4\"           # Quantization type (fp4 or nf4)\nbnb_4bit_compute_dtype=torch.float16    # Compute dtype for 4-bit base models\n#compute_dtype = getattr(torch, bnb_4bit_compute_dtype)\nuse_nested_quant = False\n\n#----Training arguments parameters-----#\noutput_dir = \"./results\"               # output directory\nnum_train_epochs = 1                   # traning epochs\nfp16 = False                           # Enable fp16/bf16 training (set bf16 to True with an A100)\nbf16 = False\nper_device_train_batch_size = 4        # Batch size per GPU for training\nper_device_eval_batch_size = 4         # Batch size per GPU for evaluation\ngradient_accumulation_steps = 1        # Number of update steps to accumulate the gradients for\ngradient_checkpointing = True\nmax_grad_norm = 0.3                    # Maximum gradient normal (gradient clipping)\nlearning_rate = 2e-4\nweight_decay = 0.01                   # Weight decay to apply to all layers except bias/LayerNorm weights\noptim = \"paged_adamw_32bit\"            # optimizer\nlr_scheduler_type = \"cosine\"           # Learning rate scheduler\nmax_steps = -1                         # Number of training steps (overrides num_train_epochs)\nwarmup_ratio = 0.03                    # Ratio of steps for a linear warmup (from 0 to learning rate)\ngroup_by_length = True                 # Group sequences into batches with same length. Saves memory and speeds up training considerably\nsave_steps = 0                         # Save checkpoint every X updates steps\nlogging_steps = 5                     # Log every X updates steps\n\n\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"bnb_config = BitsAndBytesConfig(\n    load_in_4bit=load_4bit,\n    bnb_4bit_quant_type=bnb_4bit_quant_type,\n    bnb_4bit_compute_dtype=bnb_4bit_compute_dtype,\n    bnb_4bit_use_double_quant=use_nested_quant,\n)\n\n# Load base model\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_name,\n    quantization_config=bnb_config,\n    device_map='auto'\n)\nmodel.config.use_cache = False\nmodel.config.pretraining_tp = 1\n\n# Load LLaMA tokenizer\n#tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n#tokenizer.pad_token = tokenizer.eos_token\n#tokenizer.padding_side = \"right\" # Fix weird overflow issue with fp16 training\n\n# Move model to GPU and wrap with DataParallel if multiple GPUs are available\n\n\n# Load LoRA configuration\npeft_config = LoraConfig(\n    lora_alpha=lora_alpha,\n    lora_dropout=lora_dropout,\n    r=lora_r,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n)\n# peft_config = LoraConfig(\n#     lora_alpha=lora_alpha,\n#     lora_dropout=lora_dropout,\n#     r=lora_r,\n#     target_modules=[\"q_proj\", \"v_proj\"],  # Example for Llama-2, adjust as needed for your model\n#     bias=\"none\",\n#     task_type=\"CAUSAL_LM\",\n# )\n# Set training parameters\ntraining_arguments = TrainingArguments(\n    output_dir=output_dir,\n    num_train_epochs=num_train_epochs,\n    per_device_train_batch_size=per_device_train_batch_size,\n    gradient_accumulation_steps=gradient_accumulation_steps,\n    optim=optim,\n    save_steps=save_steps,\n    logging_steps=logging_steps,\n    learning_rate=learning_rate,\n    weight_decay=weight_decay,\n    fp16=fp16,\n    bf16=bf16,\n    max_grad_norm=max_grad_norm,\n    max_steps=max_steps,\n    warmup_ratio=warmup_ratio,\n    group_by_length=group_by_length,\n    lr_scheduler_type=lr_scheduler_type,\n    report_to=\"tensorboard\",\n    eval_steps=5,  # Number of steps between evaluations\n    load_best_model_at_end=True,\n    save_strategy=\"epoch\", \n    evaluation_strategy=\"epoch\",\n)\n\n# df_read = pd.read_csv(\"/content/conversational_genAI.csv\")\n# Ensure the key exists in the DataFrame\n#if \"formatted_instruction\" not in df_read.columns:\n#    raise ValueError(\"The key 'formatted_instruction' does not exist in the DataFrame\")\n# Assuming you want to create a DatasetDict with the key \"formatted_instruction\"\n#dataset_dict = DatasetDict({\"formatted_instruction\": dataset})\n\n# save_metrics_callback = SaveMetricsCallback(save_path=\"training_metrics.csv\")\n\n# Set supervised fine-tuning parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=tokenized_dataset[\"train\"],\n    eval_dataset=tokenized_dataset[\"validation\"],\n    peft_config=peft_config,\n    dataset_text_field=\"formatted_instruction\",\n    max_seq_length=max_seq_length,\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=packing,\n#     callbacks=[save_metrics_callback]\n    callbacks=[csv_logger]\n)\nimport gc\ngc.collect()\ntorch.cuda.empty_cache()\n\nimport os\nimport csv\n\ncsv_logger = CSVLoggerCallback(file_name='training_progress.csv')\ntrainer.train()\n\nif isinstance(trainer.model, DataParallel):\n    trainer.model.module.save_pretrained(new_model)\nelse:\n    trainer.model.save_pretrained(new_model)\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}